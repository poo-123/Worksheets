WORKSHEET
MACHINE LEARNING – WORKSHEET 4

In Q1 to Q8, only one option is correct, Choose the correct option:

1. Which of the following in sklearn library is used for hyper parameter tuning?

A) GridSearchCV() B) RandomizedCV()
C) K-fold Cross Validation D) None of the above

ANS: A) GridSearchCV

2. In which of the below ensemble techniques trees are trained in parallel?

A) Random forest B) Adaboost
C) Gradient Boosting D) All of the above

ANS: A) Random forest 

3. In machine learning, if in the below line of code:
 sklearn.svm.SVC (C=1.0, kernel='rbf', degree=3)
we increasing the C hyper parameter, what will happen?

A) The regularization will increase B) The regularization will decrease
C) No effect on regularization D) kernel will be changed to linear

ANS: A) The regularization will increase

4. Check the below line of code and answer the following questions:
 sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None,
min_samples_split=2)
Which of the following is true regarding max_depth hyper parameter?

A) It regularizes the decision tree by limiting the maximum depth up to which a tree can be grown.
B) It denotes the number of children a node can have.
C) both A & B
D) None of the above

ANS: A) It regularizes the decision tree by limiting the maximum depth up to which a tree can be grown.

5. Which of the following is true regarding Random Forests?

A) It's an ensemble of weak learners.
B) The component trees are trained in series
C) In case of classification problem, the prediction is made by taking mode of the class labels predicted by the
component trees.
D)None of the above

ANS: A) It's an ensemble of weak learners.

6. What can be the disadvantage if the learning rate is very high in gradient descent?

A) Gradient Descent algorithm can diverge from the optimal solution.
B) Gradient Descent algorithm can keep oscillating around the optimal solution and may not settle.
C) Both of them
D)None of them.

ANS: C) Both of them

7. As the model complexity increases, what will happen?

A) Bias will increase, Variance decrease B) Bias will decrease, Variance increase
C)both bias and variance increase D) Both bias and variance decrease.

ANS: B) Bias will decrease, Variance increase

8. Suppose I have a linear regression model which is performing as follows:
 Train accuracy=0.95
 Test accuracy=0.75

Which of the following is true regarding the model?
A) model is underfitting B) model is overfitting
C) model is performing good D) None of the above

ANS:  B) model is overfitting

Q9 to Q15 are subjective answer type questions, Answer them briefly.

9. Suppose we have a dataset which have two classes A and B. The percentage of class A is 40% and
percentage of class B is 60%. Calculate the Gini index and entropy of the dataset.

10. What are the advantages of Random Forests over Decision Tree?

ANS: Advantages of Random Forests over Decision Tree:
1. Random forests consist of multiple single trees each based on a random sample of the training data. They are typically more accurate than single decision trees. 
2. Trees are unpruned- While a single decision tree like CART is often pruned, a random forest tree is fully grown and unpruned, and so, naturally, the feature space is split into more and smaller regions.
3. Trees are diverse- Each random forest tree is learned on a random sample, and at each node, a random set of features are considered for splitting.
4. A single decision tree needs pruning to avoid overfitting. The following shows the decision boundary from an unpruned tree. The boundary is smoother but makes obvious mistakes (overfitting).
The randomness and voting mechanisms in random forests elegantly solve the overfitting problem.


11. What is the need of scaling all numerical features in a dataset? Name any two techniques used for scaling.

ANS: Feature Scaling : It is a step of Data Pre Processing which is applied to independent variables or features of data. It basically helps to normalise the data within a particular range. 
Sometimes, it also helps in speeding up the calculations in an algorithm.
Most of the times, dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem.
If left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, like :5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.
To supress this effect, we need to bring all features to the same level of magnitudes. This can be acheived by scaling.
Techniques used for scaling:
1. Standardisation
2. Min-Max Scaling

WORKSHEET

12. Write down some advantages which scaling provides in optimization using gradient descent algorithm.

ANS: Optimization refers to the task of minimizing/maximizing an objective function f(x) parameterized by x. In machine/deep learning terminology, it’s the task of minimizing the cost/loss function J(w) parameterized by the model’s parameters w ∈ R^d.
Gradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters.
On each iteration, we update the parameters in the opposite direction of the gradient of the objective function J(w) w.r.t the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate α.
Therefore, we follow the direction of the slope downhill until we reach a local minimum.
let’s assume that the logistic regression model has only two parameters: weight w and bias b.
1. Initialize weight w and bias b to any random numbers.
2. Pick a value for the learning rate α. The learning rate determines how big the step would be on each iteration.
*If α is very small, it would take long time to converge and become computationally expensive.
*If α is large, it may fail to converge and overshoot the minimum.
Therefore, plot the cost function against different values of α and pick the value of α that is right before the first value that didn’t converge so that we would have a very fast learning algorithm that converges.
3. Make sure to scale the data if it’s on a very different scales. If we don’t scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge.

13. In case of a highly imbalanced dataset for a classification problem, is accuracy a good metric to measure the
performance of the model. If not, why?

ANS: A highly imbalanced dataset for a classification problem, accuracy is not a good metric to measure the
performance of the model.
Classification accuracy involves first using a classification model to make a prediction for each example in a test dataset. The predictions are then compared to the known labels for those examples in the test set. 
Accuracy is then calculated as the proportion of examples in the test set that were predicted correctly, divided by all predictions that were made on the test set.
	Accuracy = Correct Predictions / Total Predictions
An imbalanced classification problem is an example of a classification problem where the distribution of examples across the known classes is biased or skewed. The distribution can vary from a slight bias to a severe imbalance where there is one example in the minority class for hundreds, thousands, or millions of examples in the majority class or classes.
Imbalanced classifications pose a challenge for predictive modeling as most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class. This results in models that have poor predictive performance, specifically for the minority class. 
This is a problem because typically, the minority class is more important and therefore the problem is more sensitive to classification errors for the minority class than the majority class.

14. What is “f-score" metric? Write its mathematical formula.

ANS: The F1 score is the harmonic mean of the precision and recall. The highest possible value of F1 is 1, indicating perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero.
F-score Formula
The formula for the standard F1-score is the harmonic mean of the precision and recall. A perfect model has an F-score of 1.
f1=2*(precision*recall/precision+recall)



15. What is the difference between fit(), transform() and fit_transform()?

ANS: 

fit() : used for generating learning model parameters from training data

transform() : parameters generated from fit() method,applied upon model to generate transformed data set.

fit_transform() : combination of fit() and transform() api on same data set

