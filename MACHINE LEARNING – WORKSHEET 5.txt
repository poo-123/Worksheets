MACHINE LEARNING

WORKSHEET – 5

In Q1 to Q7, only one option is correct, Choose the correct option:

1. What is the advantage of hierarchical clustering over K-means clustering?

A) hierarchical clustering is computationally less expensive
B) in hierarchical clustering you don’t need to assign number of clusters in beginning
C) Both are equally proficient D) None of these

ANS: B) in hierarchical clustering you don’t need to assign number of clusters in beginning

2. Which of the following hyper parameter(s), when increased may cause random forest to over fit the data?

A) max_depth B) entropy
C) min_samples_leaf D) min_samples_splits

ANS: A) max_depth 

3. Which of the following is the least preferable resampling method in handling imbalance datasets?

A) SMOTE B) RandomOverSampler
C) RandomUnderSampler D) ADASYN

ANS: D) ADASYN

4. Which of the following statements is/are true about “Type-1” and “Type-2” errors?

1. Type1 is known as false positive and Type2 is known as false negative.
2. Type1 is known as false negative and Type2 is known as false positive.
3. Type1 error occurs when we reject a null hypothesis when it is actually true.
A) 1 and 2 B) 1 only
C) 1 and 3 D) 2 and 3

ANS: C) 1 and 3

5. Arrange the steps of k-means algorithm in the order in which they occur:

1. Randomly selecting the cluster centroids
2. Updating the cluster centroids iteratively
3. Assigning the cluster points to their nearest centre
A) 3-1-2 B) 2-1-3
C) 3-2-1 D) 1-3-2

ANS: B) 2-1-3

6. Which of the following algorithms is not advisable to use when you have limited CPU resources and time, and
when the data set is relatively large?

A) Decision Trees B) Support Vector Machines
C) K-Nearest Neighbours D) Logistic Regression

ANS:D) Logistic Regression

7. What is the main difference between CART (Classification and Regression Trees) and CHAID (Chi Square
Automatic Interaction Detection) Trees?

A) CART is used for classification, and CHAID is used for regression.
B) CART can create multiway trees (more than two children for a node), and CHAID can only create binary
trees (a maximum of two children for a node).
C) CART can only create binary trees (a maximum of two children for a node), and CHAID can create multiway
trees (more than two children for a node)
D) None of the above

ANS: C) CART can only create binary trees (a maximum of two children for a node), and CHAID can create multiway
trees (more than two children for a node)

In Q8 to Q10, more than one options are correct, Choose all the correct options:

8. In Ridge and Lasso regularisation if you take a large value of regularisation constant(lambda), which of the
following things may occur?

A) Ridge will lead to some of the coefficients to be very close to 0
B) Lasso will lead to some of the coefficients to be very close to 0
C) Ridge will cause some of the coefficients to become 0
D) Lasso will cause some of the coefficients to become 0

ANS: B) Lasso will lead to some of the coefficients to be very close to 0
C) Ridge will cause some of the coefficients to become 0

9. Which of the following methods can be used when there are correlated features in the dataset?

A) remove both features from the dataset
B) remove only one of the features
C) Use ridge regularisation D) use Lasso regularisation

ANS: B) remove only one of the features

10. After using linear regression, we find that the bias is very low, while the variance is very high. What are the
possible reasons for this?

A) Overfitting B) Multicollinearity
C) Underfitting D) Outliers

ANS: A) Overfitting

Q10 to Q15 are subjective answer type questions, Answer them briefly.

11. In which situation One-hot encoding must be avoided? Which encoding technique can be used in such a case?

ANS: One-Hot Encoding results in a Dummy Variable Trap as the outcome of one variable can easily be predicted with the help of the remaining variables.
Dummy Variable Trap is a scenario in which variables are highly correlated to each other.The Dummy Variable Trap leads to the problem known as multicollinearity.
Multicollinearity occurs where there is a dependency between the independent features. Multicollinearity is a serious issue in machine learning models like Linear Regression and Logistic Regression.
So, in order to overcome the problem of multicollinearity, one of the dummy variables has to be dropped.
We apply Label Encoding when:
	1.The categorical feature is ordinal (like Jr. kg, Sr. kg, Primary school, high school)
 	2.The number of categories is quite large as one-hot encoding can lead to high memory consumption.

12. In case of data imbalance problem in classification, what techniques can be used to balance the dataset? Explain
them briefly.

ANS:Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally.
For example, we may have a 2-class (binary) classification problem with 100 instances (rows). A total of 80 instances are labeled with Class-1 and the remaining 20 instances are labeled with Class-2.
This is an imbalanced dataset and the ratio of Class-1 to Class-2 instances is 80:20 or more concisely 4:1.
	A widely adopted technique for dealing with highly unbalanced datasets is called resampling.
It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling).
	Oversampling: This method adds copies of instances from the under-represented class (minority class) to obtain a balanced dataset.
	Undersampling methods: These methods simply delete instances from the over-represented class (majority class) in different ways. The most obvious way is to do delete instances randomly.
	NearMiss Undersampling: The NearMiss algorithm has been proposed to solve the issue of potential information loss. It’s based on the nearest neighbor algorithm.
	SMOTE Oversampling: SMOTE stands for Synthetic Minority Oversampling Technique — it consists of creating or synthesizing elements or samples from the minority class rather than creating copies based on those that exist already. This is used to avoid model overfitting.
	ADASYN Oversampling: ADASYN stands for Adaptive Synthetic sampling, and as SMOTE does, ADASYN generates samples of the minority class. But here, because of their density distributions, this technique receives wide attention.
			Its purpose is to generate data for minority class samples that are harder to learn, as compared to those minority samples that are easier to learn.
	
 
13. What is the difference between SMOTE and ADASYN sampling techniques?

ANS:
SMOTE: Synthetic Minority Over sampling Technique (SMOTE) algorithm applies KNN approach where it selects K nearest neighbors, joins them and creates the synthetic samples in the space. The algorithm takes the feature vectors and its nearest neighbors, computes the distance between these vectors. The difference is multiplied by random number between (0, 1) and it is added back to feature.SMOTE algorithm is a pioneer algorithm and many other algorithms are derived from SMOTE.
ADASYN:  ADAptive SYNthetic (ADASYN) is based on the idea of adaptively generating minority data samples according to their distributions using K nearest neighbor. The algorithm adaptively updates the distribution and there are no assumptions made for the underlying distribution of the data.  The algorithm uses Euclidean distance for KNN Algorithm.
	The key difference between ADASYN and SMOTE is that the former uses a density distribution, as a criterion to automatically decide the number of synthetic samples that must be generated for each minority sample by adaptively changing the weights of the different minority samples to compensate for the skewed distributions. The latter generates the same number of synthetic samples for each original minority sample.
 
14. What is the purpose of using GridsearchCV? Is it preferable to use in case of large datasets? Why or why not?

ANS: Grid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.
GridSearchCV is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters.
GridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each combination using the Cross-Validation method.
Hence after using this function we get accuracy/loss for every combination of hyperparameters and we can choose the one with the best performance.


15. List down some of the evaluation metric used to evaluate a regression model. Explain each of them in brief.

ANS: The various metrics used to evaluate the results of the prediction are :
	1.Mean Squared Error(MSE)
	2.Root-Mean-Squared-Error(RMSE).
	3.Mean-Absolute-Error(MAE).
	4.R² or Coefficient of Determination.
	5.Adjusted R²
1.Mean Squared Error(MSE): MSE or Mean Squared Error is one of the most preferred metrics for regression tasks. It is simply the average of the squared difference between the target value and the value predicted by the regression model. As it squares the differences, it penalizes even a small error which leads to over-estimation of how bad the model is. It is preferred more than other metrics because it is differentiable and hence can be optimized better.
2.Root Mean Squared Error: RMSE is the most widely used metric for regression tasks and is the square root of the averaged squared difference between the target value and the value predicted by the model. It is preferred more in some cases because the errors are first squared before averaging which poses a high penalty on large errors. This implies that RMSE is useful when large errors are undesired.
3.Mean Absolute Error: MAE is the absolute difference between the target value and the value predicted by the model. The MAE is more robust to outliers and does not penalize the errors as extremely as mse. MAE is a linear score which means all the individual differences are weighted equally. It is not suitable for applications where you want to pay more attention to the outliers.
4.R² Error: Coefficient of Determination or R² is another metric used for evaluating the performance of a regression model. The metric helps us to compare our current model with a constant baseline and tells us how much our model is better. The constant baseline is chosen by taking the mean of the data and drawing a line at the mean. R² is a scale-free score that implies it doesn't matter whether the values are too large or too small, the R² will always be less than or equal to 1.
5.Adjusted R²: Adjusted R² depicts the same meaning as R² but is an improvement of it. R² suffers from the problem that the scores improve on increasing terms even though the model is not improving which may misguide the researcher. Adjusted R² is always lower than R² as it adjusts for the increasing predictors and only shows improvement if there is a real improvement.